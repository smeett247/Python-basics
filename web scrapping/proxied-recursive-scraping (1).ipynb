{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import subprocess\n",
    "from urllib.parse import urlparse, urlsplit, urljoin, quote_plus\n",
    "import markdown\n",
    "import pdfkit\n",
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import time\n",
    "import random\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.DEBUG)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(\"scraping.log\")\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(stdout_handler)\n",
    "\n",
    "attachment_extensions = ['pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx']\n",
    "\n",
    "markdown_files = []\n",
    "attachment_files = []\n",
    "\n",
    "def save_extensions(url: str, content: bytes, folder: str, extensions: list[str], company_name: str):\n",
    "    folder_dir = os.path.join(os.getcwd(), folder, company_name)\n",
    "    if not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir)\n",
    "\n",
    "    file_extension = url.split(\".\")[-1].lower()\n",
    "    if file_extension in extensions:\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "        file_path = os.path.join(folder_dir, file_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            base_name, ext = os.path.splitext(file_name)\n",
    "            counter = 1\n",
    "            while os.path.exists(file_path):\n",
    "                file_path = os.path.join(folder_dir, f\"{base_name}_{counter}{ext}\")\n",
    "                counter += 1\n",
    "\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(content)\n",
    "        logger.info(f\"Downloaded attachment {file_path}\")\n",
    "\n",
    "        attachment_files.append(file_path)\n",
    "\n",
    "def generate_page_report(url: str, content: bytes, company_name: str):\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "\n",
    "    title = soup.title.string.strip() if soup.title and soup.title.string else \"No Title Found\"\n",
    "\n",
    "    description_meta = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    if description_meta and description_meta.get(\"content\"):\n",
    "        description = description_meta[\"content\"].strip()\n",
    "    else:\n",
    "        description = \"No Description Found\"\n",
    "\n",
    "    body_text = \"\\n\".join([p.get_text(strip=True) for p in soup.find_all(\"p\")])\n",
    "\n",
    "    report_content = f\"\"\"# {title}\n",
    "\n",
    "##### URL: [{url}]({url})\n",
    "\n",
    "**Description**: {description}\n",
    "\n",
    "**Body Text**:\n",
    "\n",
    "{body_text}\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    url_domain = urlsplit(url).netloc\n",
    "\n",
    "    if not url_domain:\n",
    "        logger.error(\"Invalid URL provided, cannot generate report filename.\")\n",
    "        return\n",
    "\n",
    "    domain_parts = url_domain.split('.')\n",
    "    if len(domain_parts) > 2:\n",
    "        domain_name = domain_parts[-2]\n",
    "    else:\n",
    "        domain_name = domain_parts[0]\n",
    "\n",
    "    reports_dir = os.path.join(\"reports\", company_name)\n",
    "    if not os.path.isdir(reports_dir):\n",
    "        os.makedirs(reports_dir)\n",
    "\n",
    "    report_filename_md = f\"{domain_name}.md\"\n",
    "    report_filepath_md = os.path.join(reports_dir, report_filename_md)\n",
    "\n",
    "    with open(report_filepath_md, \"a\", encoding='utf-8', errors='ignore') as report_file:\n",
    "        report_file.write(report_content)\n",
    "\n",
    "    logger.info(f\"Report for {url} has been appended to {report_filename_md}\")\n",
    "\n",
    "    if report_filepath_md not in markdown_files:\n",
    "        markdown_files.append(report_filepath_md)\n",
    "\n",
    "def scrape_entire_website(start_url: str, company_name: str, max_iterations=1000, max_retries=5):\n",
    "    parsed_start_url = urlparse(start_url)\n",
    "    base_domain = parsed_start_url.netloc\n",
    "\n",
    "    urls_to_scrape = [start_url]\n",
    "    scraped_urls = set()\n",
    "    iteration_count = 0\n",
    "\n",
    "    SCRAPERAPI_KEY = \"08dbecc1d0f59d9b378cf7a3b92a67b9\"\n",
    "\n",
    "    while urls_to_scrape:\n",
    "        if iteration_count >= max_iterations:\n",
    "            logger.info(f\"Maximum iteration count of {max_iterations} reached. Stopping scraping.\")\n",
    "            break\n",
    "\n",
    "        url = urls_to_scrape.pop(0)\n",
    "        if url in scraped_urls:\n",
    "            logger.debug(f\"URL already scraped: {url}\")\n",
    "            continue\n",
    "\n",
    "        success = False\n",
    "        retries = 0\n",
    "\n",
    "        while not success and retries < max_retries:\n",
    "            encoded_url = quote_plus(url)\n",
    "            scraperapi_url = f\"http://api.scraperapi.com?api_key={SCRAPERAPI_KEY}&url={encoded_url}\"\n",
    "\n",
    "            logger.info(f\"Fetching URL: {url} via ScraperAPI\")\n",
    "\n",
    "            try:\n",
    "                with httpx.Client(timeout=30) as client:\n",
    "                    r = client.get(scraperapi_url)\n",
    "                    r.raise_for_status()\n",
    "                success = True\n",
    "                logger.info(f\"Successfully fetched {url}\")\n",
    "            except httpx.RequestError as e:\n",
    "                logger.error(f\"Request error fetching {url}: {e}\")\n",
    "            except httpx.HTTPStatusError as e:\n",
    "                logger.error(f\"HTTP error fetching {url}: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error fetching {url}: {e}\")\n",
    "\n",
    "            if not success:\n",
    "                retries += 1\n",
    "                backoff_time = min(60, (2 ** retries) + random.uniform(0, 1))\n",
    "                logger.info(f\"Retrying ({retries}/{max_retries}) for URL: {url} after {backoff_time:.2f} seconds.\")\n",
    "                time.sleep(backoff_time)\n",
    "\n",
    "        if not success:\n",
    "            logger.error(f\"Failed to fetch {url} after {max_retries} retries. Skipping.\")\n",
    "            scraped_urls.add(url)\n",
    "            continue\n",
    "\n",
    "        scraped_urls.add(url)\n",
    "        iteration_count += 1\n",
    "\n",
    "        content_type = r.headers.get('Content-Type', '').lower()\n",
    "\n",
    "        if any(url.lower().endswith(f\".{ext}\") for ext in attachment_extensions):\n",
    "            save_extensions(url, r.content, \"attachments\", attachment_extensions, company_name)\n",
    "            continue\n",
    "\n",
    "        if 'text/html' in content_type:\n",
    "            generate_page_report(url, r.content, company_name)\n",
    "        else:\n",
    "            logger.info(f\"Non-HTML content at {url}, skipping parsing.\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "        new_urls = set()\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a['href']\n",
    "            joined_url = urljoin(url, href)\n",
    "            parsed_joined_url = urlparse(joined_url)\n",
    "\n",
    "            if parsed_joined_url.netloc == base_domain:\n",
    "                if joined_url not in scraped_urls and joined_url not in urls_to_scrape:\n",
    "                    new_urls.add(joined_url)\n",
    "\n",
    "        logger.info(f\"Found {len(new_urls)} new URLs on {url}.\")\n",
    "\n",
    "        urls_to_scrape.extend(new_urls)\n",
    "\n",
    "def convert_markdown_files_to_pdf():\n",
    "    for markdown_file_path in set(markdown_files):\n",
    "        try:\n",
    "            with open(markdown_file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "                html_content = markdown.markdown(text)\n",
    "\n",
    "            pdf_file_path = markdown_file_path.replace('.md', '.pdf')\n",
    "\n",
    "            path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "            config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "\n",
    "            css = \"\"\"\n",
    "            <style>\n",
    "                body {\n",
    "                    font-family: 'Calibre', sans-serif;\n",
    "                }\n",
    "            </style>\n",
    "            \"\"\"\n",
    "            full_html = css + html_content\n",
    "\n",
    "            options = {\n",
    "                'encoding': 'UTF-8',\n",
    "                'quiet': ''\n",
    "            }\n",
    "\n",
    "            pdfkit.from_string(full_html, pdf_file_path, configuration=config, options=options)\n",
    "\n",
    "            logger.info(f\"Converted {markdown_file_path} to PDF at {pdf_file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting {markdown_file_path} to PDF: {e}\")\n",
    "\n",
    "def convert_attachments_to_pdf():\n",
    "    for file_path in set(attachment_files):\n",
    "        try:\n",
    "            file_extension = file_path.split(\".\")[-1].lower()\n",
    "            pdf_file_path = file_path.rsplit('.', 1)[0] + '.pdf'\n",
    "\n",
    "            if file_extension in ['doc', 'docx', 'pptx', 'ppt']:\n",
    "\n",
    "                result = subprocess.run(['libreoffice', '--headless', '--convert-to', 'pdf', '--outdir', os.path.dirname(file_path), file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "                if result.returncode == 0:\n",
    "                    logger.info(f\"Converted {file_path} to PDF at {pdf_file_path}\")\n",
    "                else:\n",
    "                    logger.error(f\"Failed to convert {file_path} to PDF. LibreOffice error: {result.stderr.decode()}\")\n",
    "            elif file_extension == 'pdf':\n",
    "                logger.info(f\"Attachment {file_path} is already a PDF.\")\n",
    "            else:\n",
    "                logger.warning(f\"Cannot convert {file_path} to PDF. Unsupported file type.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting attachment {file_path} to PDF: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = \"https://www.scan.co.uk/shop/music-and-pro-audio\"\n",
    "company_name = \"Scan Pro Audio\"\n",
    "scrape_entire_website(start_url, company_name)\n",
    "convert_markdown_files_to_pdf()\n",
    "convert_attachments_to_pdf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
